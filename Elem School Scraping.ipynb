{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and Building the Washington State Elementary Schools Data\n",
    "\n",
    "This notebook details the process I used to build the Washington State elementary schools test scores and demographics data set, that I then used to create the Student Independent Performance metric.\n",
    "\n",
    "The data were taken from the website schooldigger.com, which has in various tables standardized test scores and demographic information for elementary, middle, and high schools across the United States. Their data is aggregated from the National Center for Education Statistics, U.S. Department of Education, the U.S. Census Bureau, the Washington State Department of Health and the Washington Office of Superindentent of Public Instruction.\n",
    "\n",
    "### Test Score Table\n",
    "\n",
    "First we'll start our Selenium web driver and navigate to the main table page for Washington elementary schools, which contain test score data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.schooldigger.com/go/WA/schoolrank.aspx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll grab the column headers, and then the first page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = html.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_columns = {}\n",
    "column_index = 0\n",
    "\n",
    "for header in tables[1].find_all(\"th\"):\n",
    "    column = \" \".join(list(header.stripped_strings))\n",
    "    table_columns[column_index] = column\n",
    "    column_index += 1\n",
    "    \n",
    "html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "tables = html.find_all('tr')\n",
    "\n",
    "data = []\n",
    "\n",
    "for table_row in tables[2:]:\n",
    "    row = {}\n",
    "    column_index = 0\n",
    "    \n",
    "    for table_data in table_row.find_all(\"td\"):\n",
    "        row[column_index] = \"\".join(list(table_data.strings)).replace('\\n', '')\n",
    "        column_index += 1\n",
    "    \n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to click the pagination buttons to navigate through the remaining pages of the table, with a little index finessing to get the data at the beginning and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_index = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(p_index < 107):\n",
    "    if p_index < 4:\n",
    "        button_index = p_index\n",
    "    else if p_index > 104:\n",
    "        button_index = p_index - 100\n",
    "    else:\n",
    "        button_index \n",
    "    \n",
    "    pagination = driver.find_element_by_class_name(\"pagination\")\n",
    "    pagination.find_elements_by_tag_name(\"a\")[button_index].click()\n",
    "\n",
    "    p_index += 1\n",
    "    \n",
    "    html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tables = html.find_all('tr')\n",
    "\n",
    "    for table_row in tables[2:]:\n",
    "        row = {}\n",
    "        column_index = 0\n",
    "    \n",
    "        for table_data in table_row.find_all(\"td\"):\n",
    "            row[column_index] = \"\".join(list(table_data.strings)).replace('\\n', '')\n",
    "            column_index += 1\n",
    "    \n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store this table in a pandas data frame for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.DataFrame(data)\n",
    "test_scores.rename(columns = table_columns, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Demographic Data\n",
    "\n",
    "Now to scrape the student demographic data, we'll need to navigate our Selenium driver to that table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagination = driver.find_element_by_class_name(\"pagination\")\n",
    "pagination.find_elements_by_tag_name(\"a\")[0].click()\n",
    "\n",
    "column_select = driver.find_element_by_class_name(\"btn-group\")\n",
    "column_select = driver.find_element_by_class_name(\"dropdown-menu\")\n",
    "cs = column_select.find_elements_by_tag_name(\"a\")\n",
    "cs[2].click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll perform the same procedure as before, first grabbing the table columns and first few rows of data, before navigating through each page for the subsequent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "tables = html.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_columns = {}\n",
    "column_index = 0\n",
    "\n",
    "for header in tables[1].find_all(\"th\"):\n",
    "    column = \" \".join(list(header.stripped_strings))\n",
    "    table_columns[column_index] = column\n",
    "    column_index += 1\n",
    "    \n",
    "html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "tables = html.find_all('tr')\n",
    "\n",
    "data = []\n",
    "\n",
    "for table_row in tables[2:]:\n",
    "    row = {}\n",
    "    column_index = 0\n",
    "    \n",
    "    for table_data in table_row.find_all(\"td\"):\n",
    "        row[column_index] = \"\".join(list(table_data.strings)).replace('\\n', '')\n",
    "        column_index += 1\n",
    "    \n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to navigate subsequent pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(p_index < 107):\n",
    "    if p_index < 4:\n",
    "        button_index = p_index\n",
    "    else if p_index > 104:\n",
    "        button_index = p_index - 100\n",
    "    else:\n",
    "        button_index \n",
    "    \n",
    "    pagination = driver.find_element_by_class_name(\"pagination\")\n",
    "    pagination.find_elements_by_tag_name(\"a\")[button_index].click()\n",
    "\n",
    "    p_index += 1\n",
    "    \n",
    "    html = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tables = html.find_all('tr')\n",
    "\n",
    "    for table_row in tables[2:]:\n",
    "        row = {}\n",
    "        column_index = 0\n",
    "    \n",
    "        for table_data in table_row.find_all(\"td\"):\n",
    "            row[column_index] = \"\".join(list(table_data.strings)).replace('\\n', '')\n",
    "            column_index += 1\n",
    "    \n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we'll store the data in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.DataFrame(data)\n",
    "students.rename(columns = table_columns, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to file\n",
    "\n",
    "Now that our data is in data frames, all that's left for us to do is join the tables and write to a csv file for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = test_scores.join(students.set_index('Rank (of 1062 )'), on = 'Rank (of 1062 )',\n",
    "                         lsuffix = \"_a\", rsuffix = \"_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"elementary_schools.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
